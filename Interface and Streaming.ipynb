{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4ZuExnsPxqaCCKk7JqwXh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Interface**\n","\n","To make it as easy as possible to create custom chains, we‚Äôve implemented a ‚ÄúRunnable‚Äù protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:\n","\n","1. **stream**: stream back chunks of the response\n","2. **invoke**: call the chain on an input\n","3. **batch**: call the chain on a list of inputs\n","\n","These also have corresponding async methods:\n","\n","1. **astream**: stream back chunks of the response async\n","2. **ainvoke**: call the chain on an input async\n","3. **abatch**: call the chain on a list of inputs async\n","4. **astream_log**: stream back intermediate steps as they happen, in addition to the final response\n","5. **astream_events**: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)"],"metadata":{"id":"5cQq5kA1EEXb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgwbEnkM-Ujh"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","model = ChatOpenAI()\n","prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n","chain = prompt | model"]},{"cell_type":"markdown","source":["**Input Schema**\n","\n","A description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation."],"metadata":{"id":"vKSZBRd7ElK0"}},{"cell_type":"code","source":["# The input schema of the chain is the input schema of its first part, the prompt.\n","chain.input_schema.schema()"],"metadata":{"id":"P9c71u7YEi8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt.input_schema.schema()"],"metadata":{"id":"Ebscaq8jEwcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.input_schema.schema()"],"metadata":{"id":"XtB5tR76Ezct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Output Schema**\n","\n","A description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation."],"metadata":{"id":"K4_DEq9oE6gW"}},{"cell_type":"code","source":["# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage\n","chain.output_schema.schema()"],"metadata":{"id":"31uK2yIUE8k6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Stream**"],"metadata":{"id":"f9m5388aFAA5"}},{"cell_type":"code","source":["for s in chain.stream({\"topic\": \"bears\"}):\n","    print(s.content, end=\"\", flush=True)"],"metadata":{"id":"jXiIM5P9FCnS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Invoke**"],"metadata":{"id":"gNCrejPgFEWQ"}},{"cell_type":"code","source":["chain.invoke({\"topic\": \"bears\"})"],"metadata":{"id":"WaPM64uSFHRK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Batch**"],"metadata":{"id":"fba31j_eFJZ7"}},{"cell_type":"code","source":["chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"],"metadata":{"id":"zy6dhEeNFJFP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Async Stream**"],"metadata":{"id":"wIZGHTbGFX8m"}},{"cell_type":"code","source":["async for s in chain.astream({\"topic\": \"bears\"}):\n","    print(s.content, end=\"\", flush=True)"],"metadata":{"id":"m8BdXSU8FT3n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Async Invoke**"],"metadata":{"id":"odI1eMPWFYbl"}},{"cell_type":"code","source":["await chain.ainvoke({\"topic\": \"bears\"})"],"metadata":{"id":"6ybuXhwqFT0o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Async Batch**"],"metadata":{"id":"YoY7YjWGFZmm"}},{"cell_type":"code","source":["await chain.abatch([{\"topic\": \"bears\"}])"],"metadata":{"id":"ytFbexMjFTZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Parallelism**\n","\n","Let‚Äôs take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel."],"metadata":{"id":"NLVo-X3NFuJI"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableParallel\n","\n","chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n","chain2 = (\n","    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")\n","    | model\n",")\n","combined = RunnableParallel(joke=chain1, poem=chain2)"],"metadata":{"id":"ZQDy2NZrFwGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain1.invoke({\"topic\": \"bears\"})"],"metadata":{"id":"BTMwFH1qF1aW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain2.invoke({\"topic\": \"bears\"})"],"metadata":{"id":"g6WlzeBaF4_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","combined.invoke({\"topic\": \"bears\"})"],"metadata":{"id":"xCREH6sdF6io"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Parallelism on batches**\n","Parallelism can be combined with other runnables. Let‚Äôs try to use parallelism with batches."],"metadata":{"id":"JplAsWNsF_jt"}},{"cell_type":"code","source":["%%time\n","chain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"],"metadata":{"id":"stsOTKvkGBCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","chain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"],"metadata":{"id":"FjV6leuCGDFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","combined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"],"metadata":{"id":"tLAouQoiGFR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Streaming With LangChain**\n","\n","Streaming is critical in making applications based on LLMs feel responsive to end-users.\n","\n","Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.\n","\n","This interface provides two general approaches to stream content:\n","\n","1. sync stream and async astream: a default implementation of streaming that streams the final output from the chain.\n","2. async astream_events and async astream_log: these provide a way to stream both intermediate steps and final output from the chain.\n","\n","Let‚Äôs take a look at both approaches, and try to understand a how to use them. ü•∑\n","\n","**Using Stream**\n","\n","All Runnable objects implement a sync method called stream and an async variant called astream.\n","\n","These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n","\n","Streaming is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n","\n","The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n","\n","The best place to start exploring streaming is with the single most important components in LLMs apps‚Äì the LLMs themselves!\n","\n","**LLMs and Chat Models**\n","\n","Large language models and their chat variants are the primary bottleneck in LLM based apps. üôä\n","\n","Large language models can take several seconds to generate a complete response to a query. This is far slower than the ~200-300 ms threshold at which an application feels responsive to an end user.\n","\n","The key strategy to make the application feel more responsive is to show intermediate progress; e.g., to stream the output from the model token by token."],"metadata":{"id":"Zlxw4kT1Miz9"}},{"cell_type":"code","source":["# Showing the example using anthropic, but you can use\n","# your favorite chat model!\n","from langchain_community.chat_models import ChatAnthropic\n","\n","model = ChatAnthropic()\n","\n","chunks = []\n","async for chunk in model.astream(\"hello. tell me something about yourself\"):\n","    chunks.append(chunk)\n","    print(chunk.content, end=\"|\", flush=True)"],"metadata":{"id":"bjQnDaqSM2W9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunks[0]"],"metadata":{"id":"hfiQ3eBBM5BS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We got back something called an AIMessageChunk. This chunk represents a part of an AIMessage.\n","\n","Message chunks are additive by design ‚Äì one can simply add them up to get the state of the response so far!"],"metadata":{"id":"0K0mMg38M9tP"}},{"cell_type":"code","source":["chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"],"metadata":{"id":"KDvmX8tUM-nk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Chains**\n","\n","Virtually all LLM applications involve more steps than just a call to a language model.\n","\n","Let‚Äôs build a simple chain using LangChain Expression Language (LCEL) that combines a prompt, model and a parser and verify that streaming works.\n","\n","We will use StrOutputParser to parse the output from the model. This is a simple parser that extracts the content field from an AIMessageChunk, giving us the token returned by the model."],"metadata":{"id":"zd7JlTzjNCgt"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n","parser = StrOutputParser()\n","chain = prompt | model | parser\n","\n","async for chunk in chain.astream({\"topic\": \"parrot\"}):\n","    print(chunk, end=\"|\", flush=True)"],"metadata":{"id":"GW0Pj3ZbNAR9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Working with Input Streams**\n","\n","What if you wanted to stream JSON from the output as it was being generated?\n","\n","If you were to rely on json.loads to parse the partial json, the parsing would fail as the partial json wouldn‚Äôt be valid json.\n","\n","You‚Äôd likely be at a complete loss of what to do and claim that it wasn‚Äôt possible to stream JSON.\n","\n","Well, turns out there is a way to do it ‚Äì the parser needs to operate on the input stream, and attempt to ‚Äúauto-complete‚Äù the partial json into a valid state.\n","\n","Let‚Äôs see such a parser in action to understand what this means."],"metadata":{"id":"V16AWadwNS52"}},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","chain = (\n","    model | JsonOutputParser()\n",")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n","async for text in chain.astream(\n","    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n","):\n","    print(text, flush=True)"],"metadata":{"id":"gHApe7hvNVTR"},"execution_count":null,"outputs":[]}]}